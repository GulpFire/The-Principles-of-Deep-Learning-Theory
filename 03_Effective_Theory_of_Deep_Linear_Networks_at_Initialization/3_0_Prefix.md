# 第三章 深度线性网络初始化时的有效理论
> ……一个具有球形对称性的系统……当然不能产生一个像马这样的生物，它不是球形对称的。
>
> —— 艾伦·图灵，关于玩具模型的局限性[24]。

在最后的热身章节中，我们介绍并解决了一个深度学习的玩具模型，即深度线性网络$｛｝^｛1｝$正如将在$\$3.1$中解释的那样，深度线性网络只是具有线性激活函数的MLP。特别地，这样的网络只能计算其输入的线性变换，并且肯定不能产生如人类这样的函数，这是经验上已知的非线性函数。尽管如此，深度线性网络的研究将成为有效的深度学习理论的有用蓝图，我们将在随后的章节中更广泛地发展该理论。具体来说，本章中的练习说明了层到层递归如何以非常直观的方式控制深度神经网络的统计数据，而不会陷入所有技术细节的泥潭。

为此，在$\$3.2$中，我们获得并精确求解深度线性网络中预激活的两点相关器的层到层递归。结果表明，网络的统计信息敏感地依赖于初始化超参数的设置，灵敏度随深度呈指数增长。这导致了关键性的重要概念，我们将在$\$5$中更深入和更敏感地探讨这一概念。简而言之，我们了解到，为了使网络表现良好，需要对这些超参数进行微调。

接下来，在$\$3.3$中，我们获得并解决四点相关器的层到层递归，尽管是针对单个输入，以进一步简化代数。这展示了网络行为依赖于架构超参数的方式，特别是网络的宽度和深度。此外，我们将四点连接相关器解释为从模型参数的绘制到绘制的网络函数波动的测量。这样的波动会干扰初始化超参数的调整，并且需要进行控制，以使网络在典型绘图时表现可靠。波动的规模由网络的深度与宽度之比设定，突出了MLP分析中的这一重要紧急规模，我们将看到，通过保持网络的深度和宽度之比足够小，可以控制波动。

---

${}^{1}$ 对于物理学家，我们给出了一个类比：深度线性网络对于深度学习就像简单谐振子对于量子力学一样。

---

最后，在$\$3.4$中，我们获得了在单个输入上评估的深度线性网络的任意$M$点相关器的递归。这种递归在任何宽度$n$和深度$L$都是完全可解的，这意味着我们可以在初始化时完全确定这些网络的统计信息$｛｝^｛2｝$给定这些非扰动解，我们采用固定深度的大宽度极限和固定宽度的大深度极限，并明确表明这两个极限不可交换。我们还构造了一个具有大宽度和大深度的内插解，但深度与宽度比$L/n$是固定的，并观察该尺度如何作为一个扰动参数，控制网络中的所有交互作用，并控制扰动分析的有效性。