# 3.1 深度线性网络

深度线性网络通过一系列简单的线性变换迭代地变换输入 $x_{i；\alpha}$

$$
\begin{aligned}
z_{i ; \alpha}^{(\ell+1)}=b_{i}^{(\ell+1)}+\sum_{j=1}^{n_{\ell}} W_{i j}^{(\ell+1)} z_{j ; \alpha}^{(\ell)},  
\end{aligned}
\tag{3.1}
$$

其中$z{i；\alpha}^｛（0）｝\equiv x_｛i；\apha｝$和$z{i；\alpha}^{（\ell）｝\equiv z{i｝^｛｛（\el）｝\left（x_｛\ alpha｝\right）$。因为线性激活函数是恒等函数，$\sigma（z）=z$，所以这里预激活和激活之间没有区别。

在本章中，我们将通过关闭所有偏差$b_｛i｝^｛（\ell）｝=0$来简化问题，这样，层$\ell$中的预激活就可以简单地通过权重矩阵的重复矩阵乘法来给出，如下所示

$$
\begin{aligned}
z_{i ; \alpha}^{(\ell)}=\sum_{j_{0}=1}^{n_{0}} \sum_{j_{1}=1}^{n_{1}} \cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}} W_{i j_{\ell-1}}^{(\ell)} W_{j_{\ell-1} j_{\ell-2}}^{(\ell-1)} \cdots W_{j_{1} j_{0}}^{(1)} x_{j_{0} ; \alpha} \equiv \sum_{j=1}^{n_{0}} \mathcal{W}_{i j}^{(\ell)} x_{j ; \alpha} .    
\end{aligned}
\tag{3.2}
$$

这里我们介绍了 $n_{\ell}$-by-$n_{0}$ 矩阵

$$
\begin{aligned}
\mathcal{W}_{i j}^{(\ell)}=\sum_{j_{1}=1}^{n_{1}} \cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}} W_{i j_{\ell-1}}^{(\ell)} W_{j_{\ell-1} j_{\ell-2}}^{(\ell-1)} \cdots W_{j_{1} j}^{(1)},    
\end{aligned}
\tag{3.3}
$$

这突出了一个事实，即第$\ell$th层的预激活只是输入的线性变换。此外，让我们设置$C_{W}^{（\ell）}\equiv C_{W}$，以使权重方差的一部分的阶数与层无关。总之，这意味着权重的初始化分布具有以下预期特征

---

$｛｝^｛2｝$这种解决的概念不应与特定学习算法的训练动态的解决相混淆。在深度线性网络的背景下，梯度下降的动力学在25中进行了分析。在10美元和10美元中，我们将在我们的有效理论形式主义的背景下解决具有一般激活函数的MLP的梯度下降的训练动力学。

---

$$
\begin{aligned}
\mathbb{E}\left[W_{i j}^{(\ell)}\right]=0, \quad \mathbb{E}\left[W_{i_{1} j_{1}}^{(\ell)} W_{i_{2} j_{2}}^{(\ell)}\right]=\delta_{i_{1} i_{2}} \delta_{j_{1} j_{2}} \frac{C_{W}}{n_{\ell-1}} .    
\end{aligned}
\tag{3.4}
$$

有点违背直觉，深度线性网络一般表示比完全一般线性变换（即具有相同输入输出维度的单层网络）更小的一组函数$｛｝^｛3｝$作为一个极端的例子，让我们以两层深度线性网络为例，其中第一个隐藏层由单个神经元$n_｛1｝=1$组成，并考虑第二层$\ell=2$中的网络输出。在这种情况下，输入中的所有信息在第一层中通过瓶颈压缩为单个数字，然后在输出层中转换为$n_{2}$维向量。当然，这样一个深度线性网络表示的线性变换子空间比所有可能的$n_{2}$乘-$n_{0}$矩阵给出的线性变换小，只要$n_{0}，n_{2}>1$。

更重要的是，我们将表明，深度线性网络在初始化时的统计数据也与单层网络的统计数据有很大不同。特别是，虽然每个$W_{ij}^（\ell）}$的统计数据由简单的高斯分布给出，但其乘积$\mathcal{W}_{ij}^（\ell）}$是非高斯的，以复杂的方式取决于网络的深度$\ell$和宽度$n_｛1｝，\ ldots，n_ \ ell｝$。

本章其余部分的目标是准确计算出这种依赖性。具体来说，我们将计算非平凡分布

$$
\begin{aligned}
p\left(z^{(\ell)} \mid \mathcal{D}\right) \equiv p\left(z^{(\ell)}\left(x_{1}\right), \ldots, z^{(\ell)}\left(x_{N_{\mathcal{D}}}\right)\right),    
\end{aligned}
\tag{3.5}
$$

当在整个数据集$\mathcal｛D｝$上求值时，迭代乘法（3.2）所暗示的预激活$z{i；\alpha}^｛（\ell）｝\equiv z{i｝^｛。如$\$1.2$中所述，分布完全由其所有$M$点相关器的集合决定，因此我们确定$p\left（z^{（\ell）}\mid\mathcal{D}\right）$的方法将是直接计算这些相关器。

在进入下一节之前，让我们考虑最简单的可观察值，即预激活$z_{i；\alph}^{（\ell）}$的平均值。通过对定义方程（3.2）的预期，很容易看出平均预激活必须在任何层消失：

$$
\begin{aligned}
\mathbb{E}\left[z_{i ; \alpha}^{(\ell)}\right] & =\sum_{j_{0}=1}^{n_{0}} \sum_{j_{1}=1}^{n_{1}} \cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}} \mathbb{E}\left[W_{i j_{\ell-1}}^{(\ell)} W_{j_{\ell-1} j_{\ell-2}}^{(\ell-1)} \cdots W_{j_{1} j_{0}}^{(1)} x_{j_{0} ; \alpha}\right] \\
& =\sum_{j_{0}=1}^{n_{0}} \sum_{j_{1}=1}^{n_{1}} \cdots \sum_{j_{\ell-1}=1}^{n_{\ell-1}} \mathbb{E}\left[W_{i j_{\ell-1}}^{(\ell)}\right] \mathbb{E}\left[W_{j_{\ell-1} j_{\ell-2}}^{(\ell-1)}\right] \cdots \mathbb{E}\left[W_{j_{1} j_{0}}^{(1)}\right] x_{j_{0} ; \alpha}=0,    
\end{aligned}
\tag{3.6}
$$

---

$｛｝^｛3｝$这不一定是一件坏事，因为关注一类特殊的函数通常既有计算上的优势，也有表示上的优势。例如，我们发现卷积网络比MLP代表的函数集要小得多，但众所周知，卷积网络在计算机视觉任务中表现得更好，这是因为它们在感应偏置方面具有平移不变性，并且由于它们的连接模式稀疏，卷积网络需要的计算量显著减少。话虽如此，与一般线性变换相比，深度线性网络是否具有有用的归纳偏差并不明显。

---

因为权重矩阵是相互独立的-并且独立于输入并且具有零均值（3.4）。通过类似的论证，很容易看出任何预激活的奇点相关器也会消失。因此，今后，我们只需要关注偶点相关器。