# 3.2 关键性

由于平均值是微不足道的，因此下一个有趣的可观测值的最简单候选是两点相关器$\mathbb｛E｝\left[z_{i_｛1｝；\alph_{1｝｝^｛（\ell）｝z_{i_ 2｝；\alpha_{2｝^｝（\eell）｝\right]$，它量化了预激活的典型幅度。我们先复习数学，然后再讨论物理。

## 数学：两点相关器的递归

让我们慢慢开始，首先考虑第一层中的两点相关器。使用定义方程（3.2）将第一层预激活表示为输入

$$
\begin{aligned}
z_{i ; \alpha}^{(1)}=\sum_{j}^{n_{0}} W_{i j}^{(1)} x_{j ; \alpha},    
\end{aligned}
\tag{3.7}
$$

我们可以将两点相关器表示为

$$
\begin{aligned}
\mathbb{E}\left[z_{i_{1} ; \alpha_{1}}^{(1)} z_{i_{2} ; \alpha_{2}}^{(1)}\right] & =\sum_{j_{1}, j_{2}=1}^{n_{0}} \mathbb{E}\left[W_{i_{1} j_{1}}^{(1)} x_{j_{1} ; \alpha_{1}} W_{i_{2} j_{2}}^{(1)} x_{j_{2} ; \alpha_{2}}\right] \\
& =\sum_{j_{1}, j_{2}=1}^{n_{0}} \mathbb{E}\left[W_{i_{1} j_{1}}^{(1)} W_{i_{2} j_{2}}^{(1)}\right] x_{j_{1} ; \alpha_{1}} x_{j_{2} ; \alpha_{2}} \\
& =\sum_{j_{1}, j_{2}=1}^{n_{0}} \frac{C_{W}}{n_{0}} \delta_{i_{1} i_{2}} \delta_{j_{1} j_{2}} x_{j_{1} ; \alpha_{1}} x_{j_{2} ; \alpha_{2}}=\delta_{i_{1} i_{2}} C_{W} \frac{1}{n_{0}} \sum_{j=1}^{n_{0}} x_{j ; \alpha_{1}} x_{j ; \alpha_{2}},    
\end{aligned}
\tag{3.8}
$$

从第二行到第三行，我们Wick收缩了两个权重并插入了方差（3.4）。此外，让我们介绍一下符号

$$
\begin{aligned}
G_{\alpha_{1} \alpha_{2}}^{(0)} \equiv \frac{1}{n_{0}} \sum_{i=1}^{n_{0}} x_{i ; \alpha_{1}} x_{i ; \alpha_{2}},    
\end{aligned}
\tag{3.9}
$$

对于两个输入的内积，由输入维度$n{0}$归一化。就这个对象而言，我们可以将第一层两点相关器$3.8$重写为

$$
\begin{aligned}
\mathbb{E}\left[z_{i_{1} ; \alpha_{1}}^{(1)} z_{i_{2} ; \alpha_{2}}^{(1)}\right]=\delta_{i_{1} i_{2}} C_{W} G_{\alpha_{1} \alpha_{2}}^{(0)} .    
\end{aligned}
\tag{3.10}
$$

接下来，我们可以无意识地重复相同的练习，以获得任意层中的两点相关器，使用定义方程（3.2）以输入表示$z_{i；\alph}^{（\ell）}$。相反，为了实践我们的递归方法，让我们递归地评估两点相关器。为此，我们归纳地假设第$\ell$-层的两点相关器是已知的，然后导出第$（\ell+1）$-层上的两点相关。使用偏差设置为零的迭代方程（3.1），我们发现

$$
\begin{aligned}
\mathbb{E}\left[z_{i_{1} ; \alpha_{1}}^{(\ell+1)} z_{i_{2} ; \alpha_{2}}^{(\ell+1)}\right] & =\sum_{j_{1}, j_{2}=1}^{n_{\ell}} \mathbb{E}\left[W_{i_{1} j_{1}}^{(\ell+1)} W_{i_{2} j_{2}}^{(\ell+1)} z_{j_{1} ; \alpha_{1}}^{(\ell)} z_{j_{2} ; \alpha_{2}}^{(\ell)}\right] \\
& =\sum_{j_{1}, j_{2}=1}^{n_{\ell}} \mathbb{E}\left[W_{i_{1} j_{1}}^{(\ell+1)} W_{i_{2} j_{2}}^{(\ell+1)}\right] \mathbb{E}\left[z_{j_{1} ; \alpha_{1}}^{(\ell)} z_{j_{2} ; \alpha_{2}}^{(\ell)}\right] \\
& =\delta_{i_{1} i_{2}} C_{W} \frac{1}{n_{\ell}} \sum_{j=1}^{n_{\ell}} \mathbb{E}\left[z_{j ; \alpha_{1}}^{(\ell)} z_{j ; \alpha_{2}}^{(\ell)}\right]    
\end{aligned}
\tag{3.11}
$$

从第一行到第二行，我们使用了这样一个事实，即第$（\ell+1）$-层的权重$W^{（\ell+1）}$在统计上独立于第$\ell$-层中的预激活$z^{。注意，在任何层，两点相关器都与Kronecker delta$\delta_{i_{1}i_{2}}$成比例，除非神经索引$i_{1}$和$i_{2}$相同，否则它们将消失。考虑到这一点，让我们将两点相关器分解为

$$
\begin{aligned}
\mathbb{E}\left[z_{i_{1} ; \alpha_{1}}^{(\ell)} z_{i_{2} ; \alpha_{2}}^{(\ell)}\right] \equiv \delta_{i_{1} i_{2}} G_{\alpha_{1} \alpha_{2}}^{(\ell)},    
\end{aligned}
\tag{3.12}
$$

并对任意层$\ell$引入上述符号（3.9）的推广。将该方程乘以$\delta_{i_{1}i_{2}}$，求和$i_{1}，i_{2}＝1，\ldots，n_｛\ell｝$，再除以$n_｛\ ell｝，$G_｛\alpha_{1}\alpha_2｝^｛（\ell）｝$也可以表示为

$$
\begin{aligned}
G_{\alpha_{1} \alpha_{2}}^{(\ell)}=\frac{1}{n_{\ell}} \sum_{j=1}^{n_{\ell}} \mathbb{E}\left[z_{j ; \alpha_{1}}^{(\ell)} z_{j ; \alpha_{2}}^{(\ell)}\right],    
\end{aligned}
\tag{3.13}
$$

因此可以认为是第$\ell$-层中预激活的平均内积，除以第$n_{\ell}$层中神经元的数量。此内积仅取决于样本索引，并允许我们在通过$\ell$层深度线性网络后，将$G_｛\lang1033{｛1｝\ alpha｛2｝｝｛（\ell）｝\ equiv G^｛（\ell）｝\left（x_｛\ alpa｛1｛｝｝｝，x_｛｛\ alpha｛｝｝｝\right）$解释为两个输入的协方差，$x_｛\ alpha｝1｝｝$和$x_ \ alph｛2}｝$。

引入并充分解释了所有这些符号后，很容易看出上面的递归（3.11）可以通过

$$
\begin{aligned}
G_{\alpha_{1} \alpha_{2}}^{(\ell+1)}=C_{W} G_{\alpha_{1} \alpha_{2}}^{(\ell)},    
\end{aligned}
\tag{3.14}
$$

其描述了协方差$G_。显然，为了将协方差从$\ell$层转换为$\ell+1$层，我们只需乘以常数$C_{W}$。初始条件$G_｛\alpha_｛1｝\alph_｛2｝｝^｛（0）｝$由两个输入$（3.9$）的内积给出，并且解是指数

$$
\begin{aligned}
G_{\alpha_{1} \alpha_{2}}^{(\ell)}=\left(C_{W}\right)^{\ell} G_{\alpha_{1} \alpha_{2}}^{(0)},    
\end{aligned}
\tag{3.15}
$$

这对于矩阵乘法的重复应用是典型的。请注意，权重方差（3.4）中的宽度$n_｛\ell｝$的因子很好地去掉了，这表明这实际上是缩放方差的正确方法。

## 物理：临界

在这一点上，我们的分析已经说明了一个有趣且非常普遍的现象。考虑到解决方案（3.15），通常会发生两种情况之一。如果$C_｛W｝>1$，则协方差呈指数增长，很快被驱动到所有输入对的固定点$G_。如果$C_｛W｝<1$，则对于所有输入对，协方差指数衰减到固定点$G_。每当一个可观测值以指数方式快速接近一个值时，我们将把这个极限值称为一个平凡的不动点。与$C_｛W｝>1$相关联的值$G_｛\alpha_｛1｝\alph_｛2｝^｛\star｝=inftty$和与$C｛W｛<1$相关联值$G_｛\aalpha_｛1｝\ alpha_｝^｝\star｛=0$是平凡不动点的主要示例。

进一步探究这一点，首先注意到输出层$L$处协方差的对角部分估计了给定输入$x_{i；\alpha}的输出的典型幅度$

$$
\begin{aligned}
G_{\alpha \alpha}^{(L)}=\mathbb{E}\left[\frac{1}{n_{L}} \sum_{j=1}^{n_{L}}\left(z_{j ; \alpha}^{(L)}\right)^{2}\right] .    
\end{aligned}
\tag{3.16}
$$

考虑到这一可观察到的情况，上述指数行为应立即发出警报，发出某种数值不稳定$\left（C_{W}>1\right）$或信息丢失$\left（C_{W}<1\right）$的信号。此外，请注意，网络输出的不同组件的目标值通常为$O（1）$数字，既不是指数大也不是指数小。因此，网络的这种指数行为应该使得学习近似期望函数变得极其困难。以这种方式，这个爆炸和消失的协方差问题是臭名昭著的爆炸和消失梯度问题的一个婴儿版本，这是基于梯度的深度网络训练的一个众所周知的障碍，我们将在$\$9$中使其更精确。

然而，我们之前的分析实际上有点太快了：如果我们调整权重方差$C_{W}$，使其精确等于1，会发生什么？这显然是初始化超参数空间中的一个特殊点，将指数增长解与指数衰减解分开。回到递归（3.14），我们看到，如果$C_{W}＝1$，那么协方差是固定的$G_｛\alpha_{1}\alpha_2}｝^｛（\ell）｝＝G_｛｛\aalpha_{1｝\alph_2}｝｝^｝（0）｝\equiv G_｛\alpha_1}\alph_2｝^｛\star｝$，即使在经过深度线性网络的许多层之后，也明显地保持了输入数据的完整协方差。这是一个真正的非平凡不动点，因为它不会使输入数据的结构指数化。因此，至少在这种启发式分析水平上，选择$C_{W}=1$似乎对于以数字稳定的方式保持输入数据的结构至关重要。更一般地说，流向非平凡不动点似乎是深度网络做任何有用事情的必要条件。

当我们微调网络的初始化超参数以使协方差避免指数行为时，我们将它们称为关键初始化超参数$｛｝^｛4｝$对于深度线性网络，临界初始化超参数$C｛W｝=1$将两种状态分开，一种状态具有指数增长的协方差（$C{W｝>1$），另一种状态则具有指数衰减的协方差（[C｛W}<1$）。当权重方差被调整到临界值$C_{W}=1$时，网络具有协方差的完美自相似性，在从一层到另一层的进化过程中保持它。

在\$5中，我们将把我们的关键性分析扩展到使用任何特定激活功能的MLP。而且，如下文所述，将网络调整到临界状态对于任何深度网络都是至关重要的，至少在不使用特殊技巧以确保信号能够稳定传播的情况下，网络才能表现良好并执行有用的任务。

---

$｛｝^｛4｝$这个词的选择是出于对统计物理学中关键现象的类比。例如，考虑一个典型的例子：由铁制成的磁铁。在高温下，铁原子的磁矩或自旋指向随机方向，导致没有任何相干磁场的顺磁性相。相比之下，在低温下，自旋反而试图集体朝着同一方向定向，从而产生具有相干磁场的铁磁相位——想想孩子们玩的“$\ccap$”形状的卡通磁铁。临界温度将这两个磁性相分离，设置到临界温度的磁体将表现出非常特殊的行为，既不是顺磁性也不是铁磁性，而是自相似性。

---