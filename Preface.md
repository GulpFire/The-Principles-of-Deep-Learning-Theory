# 前言
> 这就需要与历史发展的路线彻底决裂，但这种决裂是有好处的，它能够使我们尽可能直接地接近新思想的方法。
>  
>  —— P. A. M. Dirac, 1930年,《量子力学原理》序言

这是一本关于深度学习理论的教科书式的研究专著。虽然这本书看起来可能与您之前看过的其他深度学习书籍有点不同，但我们向您保证，它适合所有具有线性代数、多变量微积分和非正式概率理论知识的人，并且对神经网络有着浓厚兴趣。无论是实践者还是理论家，我们都希望大家都能喜欢这本书。现在，让我们告诉你一些事情。

首先也是最重要的一点，在这本书中，我们在所做的每一个选择中都侧重于直觉的培养，而非形式。这并不意味着计算不完整或草率；恰恰相反，我们试图提供每项计算的全部细节，并特别强调执行相关计算所需的数学工具。事实上，了解计算是如何进行的与了解其结果一样重要，因此我们的教学重点往往是其中的细节。

第二，当本书展示所有计算的细节时，本书并没有公布在作者的电脑上的实验验证。这样做的原因很简单：从解释推导中可以学到很多东西，但从打印一个显示两条曲线重叠的验证图中可以学到的东西却不多。考虑到现代深度学习代码的简单性和计算的可用性，读者很容易自己验证任何公式；作者当然已经用绘图的方法彻底检查了这些公式，所以如果知道这个细节能让你感到欣慰，至少要知道它们确实存在于作者的个人和基于云的硬盘上。

第三，本书们的主要关注点是深度学习社区在实践中使用的现实模型：本书希望研究深度神经网络。特别是，这意味着（i）将不讨论单隐层网络上的一些特殊结果，以及（ii）神经网络的无限宽度限制（对应于零隐层网络）将仅作为起点引入。所有这些理想化的模型最终都会被扰动，直到它们与真实模型相对应。我们当然承认，有一个充满活力的深度学习理论家群体致力于探索不同类型的理想化理论极限。然而，我们的兴趣坚定地集中在为从业者所使用的工具和方法提供解释上，以努力阐明是什么使它们如此有效。

第四，本书的很大一部分集中在深层多层感知器上。我们做出这一选择是为了从教学上说明有效的理论框架的力量——而不是由于任何技术障碍——同时我们也为如何将这种形式主义扩展到其他感兴趣的架构提供了指导。事实上，我们希望我们的许多结果具有广泛的适用性，我们已经尝试将重点放在我们希望对深度学习社区具有持久和普遍价值的方面。

第五，虽然很多材料都是新颖的，并且在本书中首次出现，尽管我们的框架、符号、语言和重点都与历史发展路线相违背，但我们也非常感谢深度学习社区。考虑到这一点，在整本书中，我们将尝试引用以前的重要贡献，重点是最近的开创性深度学习成果，而不是完全全面。在我们引用的作品中，很容易找到感兴趣的人的其他参考资料。

第六，这本书最初源于与Boris Hanin合作的一个研究项目。为了说明他的努力和支持，我们相应地在封面上纪念了他。更广泛地说，我们对Rafael Araujo、Léon Bottou、Paul Dirac、Ethan Dyer、John Frank、Ross Girshick、Vince Higgs、Yoni Kahn、Yann LeCun、Kyle Mahowald、Eric Mintun、Xiaoliang Qi、Mike Rabbat、David Schwab、Stephen Shenker、Eva Silverstein、PJ Steiner、，DJ Strouse和Jesse Thaler。在组织上，我们感谢FAIR和Facebook、Diffeo和Salesforce、MIT和IAIFI、剑桥大学出版社和arXiv

第七，考虑到撰写这本书所需要的强烈（以及各种不确定的）时空和能量动量承诺，Aya, Lumi, and Lisa Yaida表示感谢；从双样本空间的角度来看，Sho 对 Adrienne Rothschilds 德表示感谢，并对本段中本应感谢的任何假设的未来 Mark 或 Emily 表示感谢。

第八，我们希望这本书传播我们的乐观主义，即有可能有一个深度学习的一般理论，这一理论既源自第一原理，又侧重于描述现实模型的实际工作方式：实践中几乎简单的现象应该对应于几乎简单的有效理论。我们梦想这种类型的思维不仅会导致更多的人工智能模型，而且会引导我们建立一个统一的框架来理解智能的普遍方面。

好像这本书的八倍序言还远远不够，请注意：这本书有一个网站，deeplearningtheory.com ，您可能想访问它以确定您刚刚发现的错误是否已经是常识。如果不是，请告诉我们。可能有馅饼。
